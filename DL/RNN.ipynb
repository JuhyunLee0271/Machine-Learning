{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPxiwcCp5Je1KvhsR8+K3mz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8VtRWtL4G-uJ"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)"]},{"cell_type":"code","source":["train_input, val_input, train_target, val_target = train_test_split (\n","    train_input, train_target, test_size=0.2, random_state=42\n",")"],"metadata":{"id":"VzZ1U7IRHSe3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토큰의 개수를 맞추기 위해 maxlen으로 패딩을 하거나 자름 \n","# 추가하거나 제거하는 대상은 앞부분 \n","train_seq = pad_sequences(train_input, maxlen=100)\n","val_seq = pad_sequences(val_input, maxlen=100)\n","train_oh = keras.utils.to_categorical(train_seq)\n","val_oh = keras.utils.to_categorical(val_seq)"],"metadata":{"id":"ZM9oQICYIkzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원-핫 인코딩 모델\n","# SimpleRNN: 순환 신경망 층\n","# input_shape = (100,500) -> 100: length, 500: one-hot 인코딩으로 표현하기 위한 배열의 길이 \n","# 토큰을 변환한 정수는 특별한 의미가 없으므로 one-hot 인코딩을 사용해야 함 (e.g., 20이 10보다 중요한 것이 아님, 즉 크기 속성을 없애는 것)\n","model = keras.Sequential()\n","model.add(keras.layers.SimpleRNN(8, input_shape=(100,500)))\n","model.add(keras.layers.Dense(1, activation='sigmoid'))\n","model.summary()"],"metadata":{"id":"iagKKF1EL9a5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습률 0.0001의 RMSprop 옵티마이저 사용\n","rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n","model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n","checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5', save_best_only=True)\n","early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","history = model.fit(train_oh, train_target, epochs=100, batch_size=64,\n","                    validation_data = (val_oh, val_target),\n","                    callbacks = [checkpoint_cb, early_stopping_cb])"],"metadata":{"id":"O2UybByfd72C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend(['train','val'])\n","plt.show()"],"metadata":{"id":"eAZzO3Foiyay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 단어 임베딩 모델 \n","# (어휘 사전의 크기, 임베딩 벡터의 크기, 샘플의 길이)\n","model2 = keras.Sequential()\n","model2.add(keras.layers.Embedding(500, 16, input_length=100))\n","model2.add(keras.layers.SimpleRNN(8))\n","model2.add(keras.layers.Dense(1, activation='sigmoid'))\n","model2.summary()"],"metadata":{"id":"s1M3iuOvk13c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n","model2.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n","checkpoint_cb = keras.callbacks.ModelCheckpoint('best-embedding-model.h5', save_best_only=True)\n","early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","history = model2.fit(train_seq, train_target, epochs=100, batch_size=64,\n","                     validation_data = (val_seq, val_target),\n","                     callbacks=[checkpoint_cb, early_stopping_cb])"],"metadata":{"id":"0qKDUzFKmSGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend(['train','val'])\n","plt.show()"],"metadata":{"id":"6ATe6-hDm_PB"},"execution_count":null,"outputs":[]}]}